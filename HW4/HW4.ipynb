{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import json\r\n",
    "from opencc import OpenCC\r\n",
    "import jieba\r\n",
    "import re\r\n",
    "from gensim.models import word2vec\r\n",
    "import numpy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r = '[）\\（\\：\\·\\，\\。\\“ \\”\\?\\？\\」\\「\\……\\、\\《\\》\\；\\)\\(]'\r\n",
    "file = ['AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK','AL']\r\n",
    "cc = OpenCC('s2t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def open_wiki():\r\n",
    "    result =[]\r\n",
    "    for n in file[0:4]:\r\n",
    "        for m in range(100):\r\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(n,str(m).zfill(2))):\r\n",
    "                data = json.loads(line)\r\n",
    "                value = re.sub(r,'',data['text'])\r\n",
    "                result.append(value)\r\n",
    "    for n in file[5:8]:\r\n",
    "        for m in range(100):\r\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(n,str(m).zfill(2))): \r\n",
    "                data = json.loads(line)\r\n",
    "                value = re.sub(r,'',data['text'])\r\n",
    "                result.append(value)\r\n",
    "    for n in file[9:11]:\r\n",
    "        for m in range(100):\r\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(n,str(m).zfill(2))):\r\n",
    "                data = json.loads(line)\r\n",
    "                value = re.sub(r,'',data['text'])\r\n",
    "                result.append(value)\r\n",
    "    for m in range(74):\r\n",
    "        for line in open('wiki_zh/{}/wiki_{}'.format('AM',str(m).zfill(2))):\r\n",
    "            data = json.loads(line)\r\n",
    "            value = re.sub(r,'',data['text'])\r\n",
    "            result.append(value)\r\n",
    "            \r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def chine(result):\r\n",
    "    value = []\r\n",
    "    for line in result:\r\n",
    "        a = cc.convert(line)\r\n",
    "        value.append(a)\r\n",
    "    return value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stopword_list():       \r\n",
    "    with open('wiki_zh/cn_stopwords.txt') as f: \r\n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\r\n",
    "    return stopword_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def word2():\r\n",
    "    train_data = word2vec.LineSentence('test.txt')\r\n",
    "    sg = 0\r\n",
    "    vector_size = 300\r\n",
    "    workers = 4\r\n",
    "    epochs = 5\r\n",
    "    model = word2vec.Word2Vec(\r\n",
    "        train_data,\r\n",
    "        vector_size=vector_size,\r\n",
    "        workers=workers,\r\n",
    "        epochs=epochs,\r\n",
    "        sg=sg,\r\n",
    "    )\r\n",
    "    model.save('word2vec.model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def macktxt(value):\r\n",
    "    stop = get_stopword_list()\r\n",
    "    f = open('test1.txt','w')\r\n",
    "\r\n",
    "    print(len(value))\r\n",
    "\r\n",
    "    jieba.set_dictionary('/usr/local/lib/python3.9/site-packages/jieba/dict.txt')\r\n",
    "    jieba.load_userdict(\"txt2.txt\")  \r\n",
    "\r\n",
    "    for line in value :  \r\n",
    "        data = jieba.lcut(line,cut_all=False)\r\n",
    "        for val in data:\r\n",
    "            if val not in stop:\r\n",
    "                f.write(val+' ')\r\n",
    "        # f.write('\\n')\r\n",
    "    f.close"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def test(string):\r\n",
    "    model = word2vec.Word2Vec.load('word2vec.model')\r\n",
    "    # print(model.wv['生物'].shape)\r\n",
    "\r\n",
    "    for item in model.wv.most_similar(string,topn=20):\r\n",
    "        print(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = open_wiki()\r\n",
    "print(len(result))\r\n",
    "value=chine(result)\r\n",
    "macktxt(value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "word2()\r\n",
    "print('訓練成功！')\r\n",
    "test('地球')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "訓練成功！\n",
      "('宇宙', 0.7954114675521851)\n",
      "('外太空', 0.7712037563323975)\n",
      "('太陽系', 0.7702351808547974)\n",
      "('火星', 0.7569582462310791)\n",
      "('母星', 0.7553830146789551)\n",
      "('星球', 0.7486514449119568)\n",
      "('木星', 0.7409071326255798)\n",
      "('冥王星', 0.7394018173217773)\n",
      "('行星', 0.7380729913711548)\n",
      "('太空', 0.7241908311843872)\n",
      "('月球', 0.712673008441925)\n",
      "('太空船', 0.708678126335144)\n",
      "('木衛二', 0.7066769003868103)\n",
      "('河系', 0.7011516094207764)\n",
      "('地球表面', 0.7004419565200806)\n",
      "('星際', 0.6934865117073059)\n",
      "('亞空間', 0.6871092915534973)\n",
      "('水星', 0.6841133236885071)\n",
      "('外星人', 0.6836854219436646)\n",
      "('土星', 0.6808692216873169)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "test('SuperJunior')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('GOT7', 0.8827243447303772)\n",
      "('EXO', 0.8793410062789917)\n",
      "('NU', 0.8497639298439026)\n",
      "('SS501', 0.8467358350753784)\n",
      "('2PM', 0.8367975354194641)\n",
      "('EST', 0.8356223106384277)\n",
      "('CNBLUE', 0.8336243629455566)\n",
      "('SISTAR', 0.826249897480011)\n",
      "('iKON', 0.8260583877563477)\n",
      "('AfterSchool', 0.8257550597190857)\n",
      "('EXID', 0.8227468132972717)\n",
      "('SHINee', 0.8206942081451416)\n",
      "('BIGBANG', 0.8200986981391907)\n",
      "('2NE1', 0.8177193403244019)\n",
      "('RedVelvet', 0.8100519180297852)\n",
      "('BoA', 0.8099263906478882)\n",
      "('BTOB', 0.8086225986480713)\n",
      "('ara', 0.8077305555343628)\n",
      "('Solo', 0.8008803129196167)\n",
      "('Happy', 0.7964335680007935)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}