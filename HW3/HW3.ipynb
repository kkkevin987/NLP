{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import re\r\n",
    "import pandas as pd\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import jieba\r\n",
    "import jieba.analyse\r\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# url = 'https://movies.yahoo.com.tw/movie_thisweek.html'\r\n",
    "\r\n",
    "# response = requests.get(url=url)\r\n",
    "\r\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\r\n",
    "\r\n",
    "# info_items = soup.find_all('div', 'release_info')\r\n",
    "\r\n",
    "# print(\"HW3 test\")\r\n",
    "\r\n",
    "# with open('本週新片.csv', 'w', encoding='big5', newline='') as csv_file:\r\n",
    "    \r\n",
    "#     csv_writer = csv.writer(csv_file)\r\n",
    "#     csv_writer.writerow(['電影片名', '電影英文片名', '上映時間', '網友期待度'])\r\n",
    "\r\n",
    "#     for item in info_items:\r\n",
    "\r\n",
    "#         name = item.find('div', 'release_movie_name').a.text.strip()\r\n",
    "#         english_name = item.find('div', 'en').a.text.strip()\r\n",
    "#         release_time = item.find('div', 'release_movie_time').text.split('：')[-1].strip()\r\n",
    "#         level = item.find('div', 'leveltext').span.text.strip()\r\n",
    "        \r\n",
    "#         csv_writer.writerow([name, english_name, release_time, level])\r\n",
    "#         print('{}({}) 上映日：{} 期待度：{}'.format(name, english_name, release_time, level))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def cutval(df):\r\n",
    "    topcut = []\r\n",
    "    for i in df['簡介']:\r\n",
    "        top = jieba.analyse.extract_tags(i,topK=2)\r\n",
    "        topcut.append(top)\r\n",
    "\r\n",
    "    typecut = []\r\n",
    "    for i in df['類型']:\r\n",
    "        ch =re.compile(\"[\\u4e00-\\u9fa5]\")\r\n",
    "        seg_word =  \"\".join(ch.findall(i))\r\n",
    "        top = jieba.lcut(seg_word)\r\n",
    "        typecut.append(top)\r\n",
    "\r\n",
    "    namecut = []\r\n",
    "    for i in df['中文名稱']:\r\n",
    "        ch =re.compile(\"[\\u4e00-\\u9fa5]\")\r\n",
    "        name =  \"\".join(ch.findall(i))\r\n",
    "        namecut.append(name)\r\n",
    "\r\n",
    "    data = { 'type':typecut ,'name':namecut,'article':topcut}\r\n",
    "    df1 = pd.DataFrame(data)\r\n",
    "\r\n",
    "    df1['type'] = df1['type'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\r\n",
    "    df1['article'] = df1['article'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\r\n",
    "    df1['key'] = df1['name'].astype(str)+' '+df1['type'].astype(str)+' '+df1['article'].astype(str)\r\n",
    "    return df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def count(df):\r\n",
    "    vectorizer = CountVectorizer()\r\n",
    "    X = vectorizer.fit_transform(df['key'])\r\n",
    "    tfidf = TfidfTransformer() \r\n",
    "    tf=tfidf.fit_transform(X)\r\n",
    "    word = vectorizer.get_feature_names() #詞袋 \r\n",
    "    \r\n",
    "    return tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def knn(X,Y):\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y['type'].str[0:2], test_size=0.07659)\r\n",
    "    clf=KNeighborsClassifier(n_neighbors=51)\r\n",
    "    clf.fit(X_train,y_train)\r\n",
    "\r\n",
    "    y_pred = clf.predict(X_test)    # 預測模型 \r\n",
    "    y_test=y_test.values\r\n",
    "\r\n",
    "    print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df = pd.read_csv('movie.csv')\r\n",
    "df = df.drop(labels=['Unnamed: 0'],axis='columns')\r\n",
    "df1 = cutval(df) \r\n",
    "tf = count(df1)\r\n",
    "knn(tf,df1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.537 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "精準度： 0.758\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}